Intro: Previously on CS401...
======

Machine Learning theorists have a probabilistic perspective. Previously we have looked at probabilistic algorithms such as:

* Bayes rule
    * P(A|B) = P(B|A) P(A) / P(B)
    * relates the odds of event A1 to event A2, before and after conditioning to another event B.
* K-means clustering
    * unsupervised learning algorithm that classifies a given data set into clusters.
* Back propagation
    * calculates the gradient of a loss function with respect to all the weights in the network.

Now: Probabilistic Estimation with Confidence Interval
======

## Augmenting the backpropagation network

### Nice to know how certain our output is

Gaussian distribution of output?

[image placeholder]

[derivation placeholder]

## Unsupervised Learning

* Data fit to probability distribution
* Graphical models
    * graph theory
    * inference is hard for graph models

[image placeholder]
